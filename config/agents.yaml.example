# Configuration-Based Agents - Example Configuration File
# =======================================================
# 
# This file demonstrates how to define custom agents and MCP servers
# using YAML configuration instead of hardcoded Python classes.
#
# To use this configuration:
# 1. Copy this file to ./config/agents.yaml
# 2. Customize the agents and MCP servers for your environment
# 3. Set AGENT_CONFIG_PATH=./config/agents.yaml in your .env file (optional - this is the default)
# 4. Set required environment variables in your .env file for template substitution
#
# IMPORTANT NOTES:
# - Configured agents, MCP servers, and chains can override built-in components by using the same ID/name
# - Each agent must handle at least one alert type and use at least one MCP server
# - Configured agents can use both built-in MCP servers (like "kubernetes-server") and configured ones
# - Alert types can only be handled by one agent (no conflicts allowed)
# - To override a built-in component, use the same server_id/agent name/chain_id in your configuration
#
# TEMPLATE VARIABLES:
# - MCP server configurations support ${VARIABLE_NAME} template syntax
# - Template variables are resolved using environment variables from your .env file
# - Built-in servers (like kubernetes-server) also support template variables
# - Missing environment variables will cause startup errors - ensure all required variables are set
# - Template variables can be used in args, command, and other string fields
#
# ITERATION STRATEGIES:
# - Each agent can use different iteration strategies for alert processing:
#   - "react": ReAct pattern with Thinkâ†’Actionâ†’Observation cycles for complete analysis
#   - "react-stage": ReAct pattern for stage-specific analysis within multi-stage chains
#   - "react-final-analysis": ReAct final analysis only, no tools, uses accumulated data
#   - "native-thinking": (Gemini 2.0+ only) Uses native thinking mode with internal reasoning,
#     native function calling, and thought signatures for multi-turn reasoning continuity.
#     NOTE: Native tools (google_search, code_execution, url_context) are NOT supported with
#     native-thinking strategy and are automatically disabled regardless of provider config.
# - Built-in agents have default strategies: KubernetesAgent uses "react"
# - Configured agents default to "react" if iteration_strategy is not specified
# - Choose "react" for standalone analysis, "react-stage" for chain stages, "react-final-analysis" for synthesis
# - Choose "native-thinking" for Gemini models when you want visible internal reasoning and reliable tool calling
#
# DATA MASKING:
# - MCP servers can include optional data_masking configuration to protect sensitive data
# - Built-in pattern groups: "basic" (api_key, password), "secrets" (basic + token), "security" (all + certificate), "kubernetes" (kubernetes_secret + basic), "all" (all patterns)
# - Custom patterns can be defined with regex pattern, replacement text, and description
# - Masking is applied to MCP server responses before reaching the LLM or logs
# - Set enabled: false to disable masking for specific servers
#
# RESULT SUMMARIZATION
# - MCP servers can include optional summarization configuration to handle large tool outputs
# - Summarization reduces context window bloat in ReAct conversations and database storage
# - Agent-provided summarizers use investigation conversation context for intelligent summarization
# - Configuration options:
#   - enabled: true/false (default: true) - Enable or disable summarization for this server
#   - size_threshold_tokens: integer (default: 5000) - Token count threshold for triggering summarization
#   - summary_max_token_limit: integer (default: 1000) - Maximum tokens allowed in summary (enforced at LLM provider level)
# - When disabled (enabled: false), all MCP tool results pass through unchanged regardless of size
# - Different servers may need different thresholds: security data often needs higher thresholds, filesystem operations often disabled
# - Summarization uses LLM provider's max_tokens parameter for guaranteed length control and cost management
#
# HIERARCHICAL ITERATION CONFIGURATION:
# - max_iterations and force_conclusion_at_max_iterations can be configured at multiple levels
# - Precedence (highest to lowest):
#   1. Parallel agent level (within agents list) - highest precedence
#   2. Stage level (within stages)
#   3. Chain level (agent_chains section)
#   4. Agent level (agents section)
#   5. System level (environment variables/settings.py) - lowest precedence
# - Each level overrides all lower levels. If not specified, inherits from the level below.
# - max_iterations: Maximum LLM->MCP iteration loops (default: 30 from system settings)
# - force_conclusion_at_max_iterations: Force conclusion vs pause when limit reached (default: false)
#
# HIERARCHICAL MCP SERVER CONFIGURATION:
# - mcp_servers can be configured at multiple levels (NEW FEATURE)
# - Precedence (highest to lowest):
#   1. Alert level (via API mcp field) - HIGHEST PRIORITY, overrides everything
#   2. Chat level (chat.mcp_servers in chain config) - for chat agents only
#   3. Parallel agent level (within agents list) - highest configuration priority
#   4. Stage level (within stages)
#   5. Chain level (agent_chains section)
#   6. Agent level (agents section) - lowest configuration priority
# - Each level overrides all lower levels. If not specified, inherits from the level below.
# - Alert-level MCP override (via API) takes precedence over ALL configuration levels
# - This allows fine-grained control: define defaults at chain level, override per-stage, or per-agent
# - Example use cases:
#   * Chain-level: Set common MCP servers for all stages (e.g., kubernetes-server)
#   * Stage-level: Add monitoring servers for specific investigation stage
#   * Parallel-agent: Each agent in parallel uses different MCP servers
#   * Chat-level: Restrict chat agent to specific MCP servers (e.g., only kubernetes, no destructive ops)
#   * Alert-level: Ad-hoc override via API for specific alert (with optional tool filtering)
#
# CHAT CONFIGURATION:
# - Chat can be configured at chain level with the "chat" section
# - Chat configuration options:
#   * enabled: true/false (default: true) - Enable/disable chat for this chain
#   * agent: "ChatAgent" (default) - Which agent to use for chat conversations
#   * iteration_strategy: Optional strategy override (defaults to last stage's strategy)
#   * llm_provider: Optional LLM provider override (defaults to chain/system provider)
#   * mcp_servers: Optional MCP server list (overrides chain-level for chat only)
#   * max_iterations: Optional iteration limit for chat (overrides chain/agent defaults)
# - Chat agents inherit chain-level configuration but can override specific settings
# - Use chat.mcp_servers to restrict chat capabilities (e.g., read-only access)
# - Use chat.max_iterations to prevent runaway chat conversations
#

# =============================================================================
# MCP SERVER CONFIGURATIONS
# =============================================================================
# Define custom MCP servers that agents can use for specialized functionality.
# These extend the built-in MCP servers and can be referenced by any agent.
#
# TRANSPORT TYPES:
# - "stdio": Command-line MCP servers (most common) - requires command field
# - "http": HTTP endpoint MCP servers - requires url field, optional bearer_token, verify_ssl
# - "sse": Server-Sent Events MCP servers - requires url field, optional bearer_token, sse_read_timeout

# ==============================================================================
# DEFAULT ALERT TYPE
# ==============================================================================
# Optional: Set the default alert type for the UI dropdown
# If not specified, defaults to "kubernetes" (built-in default)
# The default alert type must be defined in at least one chain below
default_alert_type: "PodCrashLoop"

# ==============================================================================
# MCP SERVERS
# ==============================================================================
mcp_servers:
  # OVERRIDE EXAMPLE: Override built-in kubernetes-server with custom kubeconfig path
  # This example shows how to override the built-in kubernetes-server to use a mounted kubeconfig
  # in Kubernetes deployments. The MCP_KUBECONFIG env var should point to a mounted Secret volume.
  # Uncomment this section to enable the override:
  #
  # kubernetes-server:
  #   server_id: "kubernetes-server"
  #   server_type: "kubernetes"
  #   enabled: true
  #   transport:
  #     type: "stdio"
  #     command: "npx"
  #     args:
  #       - "-y"
  #       - "kubernetes-mcp-server@0.0.54"
  #       - "--read-only"
  #       - "--disable-destructive"
  #       - "--kubeconfig"
  #       - "${MCP_KUBECONFIG}"
  #   instructions: |
  #     For Kubernetes operations:
  #     - Be careful with cluster-scoped resource listings in large clusters
  #     - Always prefer namespaced queries when possible
  #     - If you get "server could not find the requested resource" error, check if you're using the namespace parameter correctly:
  #       * Cluster-scoped resources (Namespace, Node, ClusterRole, PersistentVolume) should NOT have a namespace parameter
  #       * Namespace-scoped resources (Pod, Deployment, Service, ConfigMap) REQUIRE a namespace parameter
  #   data_masking:
  #     enabled: true
  #     pattern_groups:
  #       - "kubernetes"
  #     patterns:
  #       - "certificate"
  #       - "token"
  #       - "email"
  
  # Security-focused MCP server for threat analysis and security operations
  security-server:
    server_id: "security-server"
    server_type: "security"
    enabled: true
    transport:
      type: "stdio"  # Required discriminator field
      # Example: Custom security MCP server command with templated arguments
      command: "/opt/security-mcp/server"
      args: ["--mode", "production", "--log-level", "info", "--token", "${SECURITY_SERVER_TOKEN}"]
      env:
        SECURITY_DB_URL: "postgresql://user:pass@security-db:5432/security"
        API_TIMEOUT: "30"
    instructions: |
      Security analysis MCP server instructions:
      - Always prioritize data security over service availability
      - Check for unauthorized access patterns in logs and metrics
      - Provide detailed security recommendations with risk assessments
      - Include compliance considerations (SOC2, GDPR, etc.) in responses
      - Escalate critical security incidents immediately
    data_masking:
      enabled: true
      pattern_groups:
        - "security"          # Built-in group: api_key, password, token, certificate
      custom_patterns:
        - name: "security_token"
          pattern: "(?i)(token|jwt|bearer)[\\\"']?\\s*[:=]\\s*[\\\"']?([A-Za-z0-9_\\-\\.]{40,})[\\\"']?"
          replacement: '"security_token": "***MASKED_SECURITY_TOKEN***"'
          description: "Security tokens and JWTs"
          enabled: true
        - name: "ssh_key"
          pattern: 'ssh-(?:rsa|dss|ed25519|ecdsa)\s+[A-Za-z0-9+/=]+'
          replacement: "***MASKED_SSH_KEY***"
          description: "SSH public keys"
          enabled: true
    # Result Summarization Configuration
    summarization:
      enabled: true                    # Enable summarization for security tool output
      size_threshold_tokens: 2500      # Higher threshold for security data (needs detailed analysis)
      summary_max_token_limit: 1200    # Allow longer summaries for security context

  # Monitoring and observability MCP server for performance analysis
  monitoring-server:
    server_id: "monitoring-server"
    server_type: "monitoring"
    enabled: true
    transport:
      type: "stdio"  # Required discriminator field  
      command: "npx"
      args: ["-y", "@example/monitoring-mcp-server@latest", "--prometheus-url", "${PROMETHEUS_URL}", "--grafana-token", "${GRAFANA_TOKEN}"]
      env:
        PROMETHEUS_URL: "http://prometheus:9090"
        GRAFANA_URL: "http://grafana:3000"
        ALERT_MANAGER_URL: "http://alertmanager:9093"
    instructions: |
      Monitoring and observability instructions:
      - Focus on performance metrics, resource utilization, and system health
      - Analyze trends and patterns in metrics data
      - Provide actionable recommendations for performance optimization
      - Include relevant dashboards and queries in responses
      - Consider both immediate fixes and long-term capacity planning
    data_masking:
      enabled: true
      pattern_groups:
        - "basic"             # Built-in group: api_key, password
    # Result Summarization Configuration
    summarization:
      enabled: true                    # Enable summarization for monitoring tool output
      size_threshold_tokens: 3000      # Higher threshold for metrics data (can be voluminous)
      summary_max_token_limit: 800     # Concise summaries for metrics and performance data

  # HTTP MCP server example (using HTTP transport)
  http-mcp-server:
    server_id: "http-mcp-server"
    server_type: "http-based"
    enabled: false  # Disabled by default - enable when HTTP endpoint is configured
    transport:
      type: "http"  # HTTP transport type
      url: "https://mcp-api.example.com/mcp"  # MCP endpoint URL
      bearer_token: "${MCP_BEARER_TOKEN}"  # Bearer token for authentication
      timeout: 30  # Request timeout in seconds
      verify_ssl: true  # SSL certificate verification (recommended for production)
      headers:
        User-Agent: "tarsy/1.0"
        Accept: "application/json"
    instructions: |
      HTTP-based MCP server instructions:
      - This server communicates via HTTP/HTTPS instead of command-line
      - Supports bearer token authentication for secure access
      - Uses JSON-RPC 2.0 protocol over HTTP as per MCP specification
      - SSL verification is recommended for production endpoints
    data_masking:
      enabled: true
      pattern_groups: ["basic"]
    summarization:
      enabled: true
      size_threshold_tokens: 4000
      summary_max_token_limit: 1000

  # SSE MCP server example (using Server-Sent Events transport)
  sse-mcp-server:
    server_id: "sse-mcp-server"
    server_type: "streaming"
    enabled: false  # Disabled by default - enable when SSE endpoint is configured
    transport:
      type: "sse"  # SSE transport type
      url: "https://streaming-mcp.example.com/sse"  # SSE endpoint URL (supports endpoint discovery)
      bearer_token: "${MCP_SSE_TOKEN}"  # Bearer token for authentication
      timeout: 30  # HTTP request timeout in seconds
      sse_read_timeout: 300  # SSE stream read timeout in seconds (longer for persistent connections)
      verify_ssl: true  # SSL certificate verification (recommended for production)
      headers:
        User-Agent: "tarsy/1.0"
        Accept: "text/event-stream"
    instructions: |
      Server-Sent Events (SSE) MCP server instructions:
      - This server uses SSE for real-time streaming communication
      - Supports long-lived persistent connections for continuous data flow
      - Uses endpoint discovery for automatic SSE endpoint configuration
      - Ideal for monitoring systems, log streaming, and real-time analytics
      - Bearer token authentication for secure persistent connections
    data_masking:
      enabled: true
      pattern_groups: ["basic"]
    summarization:
      enabled: true
      size_threshold_tokens: 3500  # Slightly lower threshold for streaming data
      summary_max_token_limit: 800   # Concise summaries for streaming content

  # Custom cloud provider MCP server (example for AWS-specific operations)
  aws-server:
    server_id: "aws-server"
    server_type: "cloud"
    enabled: false  # Disabled by default - enable when AWS credentials are configured
    transport:
      type: "stdio"  # Required discriminator field
      command: "/usr/local/bin/aws-mcp-server"
      args: ["--region", "${AWS_DEFAULT_REGION}", "--access-key", "${AWS_ACCESS_KEY_ID}"]
      env:
        AWS_PROFILE: "production"
        AWS_DEFAULT_REGION: "us-west-2"
    instructions: |
      AWS cloud operations instructions:
      - Use AWS CLI and APIs for infrastructure management
      - Always verify resource states before making changes  
      - Include cost implications in recommendations
      - Follow AWS Well-Architected Framework principles
      - Ensure proper IAM permissions and security groups
    data_masking:
      enabled: false          # No masking for raw AWS data - handled by AWS IAM permissions
    # Result Summarization Configuration
    summarization:
      enabled: false                    # Disable summarization for AWS tool output

# =============================================================================
# AGENT CONFIGURATIONS
# =============================================================================
# Define custom agents that handle specific alert types using configured MCP servers.
# Each agent specifies which alert types it handles and which MCP servers it uses.

agents:
  # Security-focused agent for handling security-related alerts
  security-agent:
    mcp_servers:
      - "security-server"         # Custom security MCP server (defined above)
      - "kubernetes-server"       # Built-in Kubernetes MCP server for cluster security
    iteration_strategy: "react"   # Use ReAct strategy for systematic security analysis
    custom_instructions: |
      You are a security-focused SRE agent specializing in cybersecurity incidents.
      
      PRIORITIES:
      1. Data security and compliance over service availability
      2. Immediate containment of security threats
      3. Detailed forensic analysis and documentation
      4. Coordination with security team and stakeholders
      
      APPROACH:
      - Immediately assess the severity and scope of security incidents
      - Take containment actions to prevent further damage
      - Gather evidence and maintain chain of custody
      - Provide clear recommendations for remediation
      - Include lessons learned and prevention strategies

  # Performance-focused agent for handling performance and resource issues
  performance-agent:
    mcp_servers:
      - "monitoring-server"       # Custom monitoring MCP server (defined above)
      - "kubernetes-server"       # Built-in Kubernetes MCP server for cluster metrics
    iteration_strategy: "react" # Use react strategy for systematic performance troubleshooting
    custom_instructions: |
      You are a performance-focused SRE agent specializing in system optimization.
      
      PRIORITIES:
      1. Service availability and performance optimization
      2. Root cause analysis of performance bottlenecks
      3. Capacity planning and resource scaling
      4. User experience impact minimization
      
      APPROACH:
      - Quickly identify the root cause of performance issues
      - Implement immediate fixes to restore service levels
      - Analyze metrics and trends to understand patterns
      - Provide scaling recommendations and capacity planning
      - Focus on both immediate resolution and long-term optimization

  # Database-focused agent for handling database-related alerts
  # Example: Agent-level iteration configuration
  database-agent:
    mcp_servers:
      - "kubernetes-server"            # Built-in Kubernetes MCP server only
    # iteration_strategy: "react"      # Optional: defaults to "react" if not specified
    max_iterations: 25                 # Agent-level override (applies unless overridden by chain/stage/parallel)
    force_conclusion_at_max_iterations: false  # Agent-level setting
    custom_instructions: |
      You are a database-focused SRE agent specializing in database operations.
      
      PRIORITIES:
      1. Data integrity and consistency
      2. Database availability and performance
      3. Backup and recovery preparedness
      4. Query optimization and indexing
      
      APPROACH:
      - Ensure data integrity is never compromised
      - Identify and resolve database performance bottlenecks
      - Monitor replication health and consistency
      - Provide database optimization recommendations
      - Coordinate with DBA team for complex issues

  # Cloud infrastructure agent (disabled by default - requires cloud credentials)
  cloud-infrastructure-agent:
    mcp_servers:
      - "aws-server"                   # Custom AWS MCP server (currently disabled)
      - "kubernetes-server"            # Built-in Kubernetes MCP server
    custom_instructions: |
      You are a cloud infrastructure-focused SRE agent specializing in AWS operations.
      
      NOTE: This agent requires the aws-server MCP server to be enabled and properly configured.
      
      PRIORITIES:
      1. Infrastructure availability and resilience
      2. Cost optimization and resource efficiency
      3. Security and compliance in the cloud
      4. Automation and infrastructure as code
      
      APPROACH:
      - Leverage AWS services for scalable, resilient solutions
      - Always consider cost implications of infrastructure changes
      - Use Infrastructure as Code (CloudFormation/Terraform) when possible
      - Follow AWS Well-Architected Framework principles
      - Ensure proper monitoring and alerting for cloud resources

# =============================================================================
# AGENT CHAIN CONFIGURATIONS  
# =============================================================================
# Define sequential agent chains that process alerts through multiple stages.
# Chains enable multi-stage workflows where agents build upon each other's work.
#
# CHAIN CONCEPTS:
# - Each chain handles specific alert types through sequential stages
# - Stages execute in order, with later stages receiving accumulated data from previous stages
# - Agents can be reused across different stages with different iteration strategies
# - Single agents are treated as 1-stage chains for unified processing
#
# STAGE ITERATION STRATEGIES:
# - "react": Standard ReAct pattern for complete analysis (default)
# - "react-stage": ReAct pattern for stage-specific analysis within multi-stage chains
# - "react-final-analysis": Pure analysis using all accumulated data (no tool calls)
# - "native-thinking": Gemini 2.0+ native thinking with internal reasoning and structured tool calls
#   NOTE: Native tools (google_search, code_execution, url_context) are always disabled for this strategy

agent_chains:
  # Simple 2-stage security incident workflow
  security-incident-chain:
    alert_types: ["SecurityBreach", "AccessViolation"]
    stages:
      - name: "evidence-collection"        # Stage 1: Gather security evidence
        agent: "security-agent"
        iteration_strategy: "react-stage" # Focus on data collection and stage analysis
      - name: "security-analysis"          # Stage 2: Analyze collected evidence  
        agent: "security-agent"
        iteration_strategy: "react"       # Full analysis with recommendations
    description: "2-stage security incident investigation and response"

  # Advanced 3-stage Kubernetes troubleshooting workflow
  # Example: Chain-level and stage-level iteration configuration
  kubernetes-deep-troubleshooting:
    alert_types: ["KubernetesCritical", "PodCrashLoop"]
    max_iterations: 20                   # Chain-level override (applies to all stages unless overridden)
    force_conclusion_at_max_iterations: true
    stages:
      - name: "system-data-collection"     # Stage 1: Collect system metrics and status
        agent: "performance-agent"
        iteration_strategy: "react-stage" # Focus on gathering comprehensive data and stage analysis
        max_iterations: 15               # Stage-level override (higher precedence than chain)
      - name: "detailed-analysis"          # Stage 2: Deep dive analysis with tools
        agent: "database-agent"            # Different agent for specialized analysis
        iteration_strategy: "react-stage" # Collect additional data + stage-specific analysis
        # Uses chain-level iteration config (20 iterations, force_conclusion=true)
      - name: "final-diagnosis"            # Stage 3: Comprehensive diagnosis and recommendations
        agent: "performance-agent"         # Back to performance agent for final analysis
        iteration_strategy: "react-final-analysis" # Pure analysis, no additional data collection
        force_conclusion_at_max_iterations: false  # Stage-level override for this setting only
    description: "3-stage deep Kubernetes troubleshooting with specialized analysis"

  # Multi-stage performance investigation chain
  performance-optimization-chain:
    alert_types: ["HighLatency", "CPUSpike", "MemoryPressure"] 
    stages:
      - name: "metrics-collection"         # Stage 1: Gather performance metrics
        agent: "performance-agent"
        iteration_strategy: "react-stage" # Data collection focus
      - name: "resource-analysis"          # Stage 2: Analyze resource utilization  
        agent: "performance-agent"
        iteration_strategy: "react-stage" # Additional data + stage-specific analysis
      - name: "optimization-recommendations" # Stage 3: Provide optimization plan
        agent: "performance-agent"
        iteration_strategy: "react-final-analysis" # Comprehensive recommendations
    description: "3-stage performance optimization with incremental analysis"

  # Complex 4-stage multi-domain incident investigation  
  cross-platform-incident-chain:
    alert_types: ["SystemFailure", "MultiComponentFailure"]
    stages:
      - name: "kubernetes-investigation"    # Stage 1: Kubernetes cluster analysis
        agent: "database-agent"             # Using database-agent for K8s analysis
        iteration_strategy: "react-stage" # Collect K8s data + initial analysis
      - name: "performance-investigation"   # Stage 2: Performance metrics analysis
        agent: "performance-agent" 
        iteration_strategy: "react-stage" # Collect perf data + analysis
      - name: "security-assessment"         # Stage 3: Security impact assessment
        agent: "security-agent"
        iteration_strategy: "react-stage" # Security data + assessment
      - name: "unified-incident-report"     # Stage 4: Comprehensive incident report
        agent: "security-agent"             # Security agent for final report (highest priority)
        iteration_strategy: "react-final-analysis" # Synthesize all data into final report
    description: "4-stage cross-platform incident investigation with unified reporting"

  # Simple single-stage chain (equivalent to direct agent usage)
  database-quick-fix:
    alert_types: ["DatabaseConnectionFailure"]
    stages:
      - name: "database-diagnosis"          # Single stage for simple issues
        agent: "database-agent"
        # iteration_strategy: "react"       # Uses agent default if not specified
    description: "Single-stage database issue resolution"

  # Native thinking chain (Gemini 2.0+ only)
  # Demonstrates native-thinking strategy with visible internal reasoning
  # NOTE: Requires Gemini 2.0+ model as LLM provider (e.g., gemini-3-pro-preview)
  native-thinking-investigation:
    alert_types: ["ComplexIncident", "DeepAnalysis"]
    stages:
      - name: "deep-investigation"          # Stage 1: Deep thinking investigation
        agent: "performance-agent"
        iteration_strategy: "native-thinking"  # Uses Gemini native thinking mode
        # Benefits: Visible internal reasoning (ðŸ§ ), reliable native function calling,
        # thought signatures for multi-turn reasoning continuity
        # NOTE: Native tools (google_search, code_execution, url_context) are automatically
        # disabled when using native-thinking strategy, regardless of provider configuration
      - name: "synthesis"                   # Stage 2: Synthesize findings
        agent: "performance-agent"
        iteration_strategy: "react-final-analysis"
    description: "2-stage investigation using Gemini native thinking for deep analysis"

  # Parallel multi-agent investigation with synthesis
  # Demonstrates parallel execution with configurable synthesis
  # Example: Parallel agent-level iteration configuration (highest precedence)
  parallel-investigation-with-synthesis:
    alert_types: ["ComplexSystemFailure", "MultiDomainIncident"]
    max_iterations: 30                    # Chain-level default
    stages:
      - name: "parallel-investigation"
        max_iterations: 25                # Stage-level override
        agents:
          - name: "KubernetesAgent"
            llm_provider: "google-default"
            iteration_strategy: "react"
            max_iterations: 20            # Parallel agent-level override (highest precedence)
            force_conclusion_at_max_iterations: true
          - name: "database-agent"
            llm_provider: "anthropic-default"
            iteration_strategy: "react"
            # Uses stage-level iteration config (25 iterations)
        # Optional synthesis configuration
        synthesis:
          agent: "SynthesisAgent"                # Optional, defaults to SynthesisAgent
          iteration_strategy: "synthesis"        # or "synthesis-native-thinking"
          llm_provider: "anthropic-default"      # Optional, uses stage/chain default if not set
    description: "Multi-agent parallel investigation with configurable synthesis"

  # Replica execution with native thinking synthesis
  # Demonstrates replica redundancy with Gemini synthesis
  replica-with-native-synthesis:
    alert_types: ["HighConfidenceRequired"]
    llm_provider: "google-default"  # Chain-level default
    stages:
      - name: "replicated-analysis"
        agent: "KubernetesAgent"
        replicas: 3
        synthesis:
          iteration_strategy: "synthesis-native-thinking"  # Use Gemini native thinking for synthesis
          # agent and llm_provider inherit defaults
    description: "Replicated analysis with Gemini native thinking synthesis"

  # Chat configuration examples
  # Demonstrates different chat configuration patterns
  custom-chat-config-chain:
    alert_types: ["CustomChatExample"]
    llm_provider: "anthropic-default"  # Chain default
    mcp_servers: ["kubernetes-server", "monitoring-server"]  # Chain-level MCP servers
    chat:
      enabled: true                         # Optional, defaults to true
      agent: "ChatAgent"                    # Optional, defaults to ChatAgent
      iteration_strategy: "native-thinking" # Optional, determined from last stage if not set
      llm_provider: "google-default"        # Optional, uses chain/system default if not set
      mcp_servers: ["kubernetes-server"]    # Optional, chat-level MCP override (overrides chain-level)
      max_iterations: 10                    # Optional, chat-level iteration limit (overrides chain/agent defaults)
    stages:
      - name: "investigation"
        agent: "KubernetesAgent"
    description: "Chain with custom chat configuration (agent, strategy, provider, MCP servers, max iterations)"

  chat-disabled-chain:
    alert_types: ["NoChatExample"]
    chat:
      enabled: false  # Explicitly disable chat for this chain
    stages:
      - name: "investigation"
        agent: "database-agent"
    description: "Chain with chat explicitly disabled"

  default-chat-chain:
    alert_types: ["DefaultChatExample"]
    # chat field not specified = enabled with all defaults
    # (ChatAgent, strategy determined from last stage, chain/system LLM provider)
    stages:
      - name: "investigation"
        agent: "KubernetesAgent"
    description: "Chain with default chat configuration (enabled, all defaults)"
  
  # ===========================================================================
  # HIERARCHICAL MCP CONFIGURATION EXAMPLE
  # ===========================================================================
  # Demonstrates the new MCP server hierarchy feature
  hierarchical-mcp-example:
    alert_types: ["MCPHierarchyExample"]
    # Chain-level MCP servers: Default for all stages unless overridden
    mcp_servers: ["kubernetes-server"]  # All stages use kubernetes-server by default
    stages:
      - name: "initial-investigation"
        agent: "KubernetesAgent"
        # This stage inherits chain-level mcp_servers: ["kubernetes-server"]
      
      - name: "detailed-analysis"
        agent: "KubernetesAgent"
        # Stage-level override: Add monitoring server for this stage only
        mcp_servers: ["kubernetes-server", "monitoring-server"]
      
      - name: "parallel-deep-dive"
        # Parallel agents with different MCP servers
        agents:
          - name: "KubernetesAgent"
            # Parallel-agent override: Highest config priority
            mcp_servers: ["kubernetes-server"]  # K8s agent uses only k8s-server
          - name: "performance-agent"
            mcp_servers: ["monitoring-server"]  # Performance agent uses only monitoring
    description: "Demonstrates hierarchical MCP server configuration at chain/stage/parallel-agent levels"
    
    # NOTE: Alert-level MCP override (via API) would take precedence over ALL of the above.
    # Example API call with alert-level override (highest priority):
    # POST /api/v1/alerts
    # {
    #   "alert_type": "MCPHierarchyExample",
    #   "data": {"pod": "my-pod"},
    #   "mcp": {
    #     "servers": [
    #       {"name": "kubernetes-server", "tools": ["get_pod_logs"]},
    #       {"name": "custom-server"}
    #     ]
    #   }
    # }
    # This alert-level mcp would override ALL chain/stage/parallel-agent mcp_servers above
# =============================================================================
# CONFIGURATION VALIDATION NOTES
# =============================================================================
#
# The following validation rules apply to this configuration:
#
# 1. UNIQUE NAMING:
#    - Agent names must not conflict with built-in agent classes
#    - MCP server IDs must not conflict with built-in MCP servers
#    - Current built-in agents: KubernetesAgent
#    - Current built-in MCP servers: kubernetes-server
#
# 2. ALERT TYPE EXCLUSIVITY:
#    - Each alert type can only be handled by one agent
#    - No conflicts allowed between configured and built-in agents
#    - Current built-in alert types: kubernetes, NamespaceTerminating
#
# 3. MCP SERVER REFERENCES:
#    - All MCP servers referenced by agents must exist (built-in or configured)
#    - MCP servers can be temporarily disabled (enabled: false) but still referenced
#    - Agents will fail to initialize if they reference non-existent MCP servers
#
# 4. REQUIRED FIELDS:
#    - Agents must have at least one alert_type and one mcp_server
#    - MCP servers must have server_id, server_type, and transport (with type field)
#    - All fields marked as required in the Pydantic models must be present
#
# 5. AGENT CHAIN VALIDATION:
#    - Chain IDs must be unique across built-in and configured chains
#    - All agents referenced in chain stages must exist (built-in or configured)
#    - Alert types can only be handled by one chain (no conflicts allowed)
#    - Each chain must have at least one stage
#    - Stage names within a chain must be unique
#
# =============================================================================
# DEPLOYMENT CHECKLIST
# =============================================================================
#
# Before deploying this configuration:
#
# 1. â–¡ Copy this file to ./config/agents.yaml
# 2. â–¡ Customize agent configurations for your environment
# 3. â–¡ Configure MCP server connection parameters
# 4. â–¡ Set required environment variables in .env for template substitution
# 5. â–¡ Test MCP server connectivity independently
# 6. â–¡ Enable only the MCP servers you have properly configured
# 7. â–¡ Verify no alert type conflicts with existing built-in agents or chains
# 8. â–¡ Configure agent chains for multi-stage workflows if needed
# 9. â–¡ Verify all agents referenced in chains exist and are properly configured
# 10. â–¡ Configure MCP result summarization thresholds for your server output patterns (EP-0015)
# 11. â–¡ Test summarization behavior with large MCP tool outputs in development
# 12. â–¡ Set AGENT_CONFIG_PATH in .env if using a different location
# 13. â–¡ Test configuration validation: python -c "from tarsy.config.agent_config import ConfigurationLoader; ConfigurationLoader('./config/agents.yaml').load_and_validate()"
# 14. â–¡ Monitor application startup logs for configuration errors
# 15. â–¡ Verify agents and chains are properly registered in the ChainRegistry
# 16. â–¡ Verify summarization interactions appear correctly in dashboard timeline
#
# ============================================================================= 
