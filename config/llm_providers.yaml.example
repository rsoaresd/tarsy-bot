# LLM Providers Configuration File with Tool Result Limits
# This file allows you to override built-in default providers or add custom providers
# Copy this file to llm_providers.yaml and customize as needed

# Built-in providers are available out-of-the-box:
# - openai-default (gpt-5) - requires OPENAI_API_KEY
# - google-default (gemini-2.5-flash) - requires GOOGLE_API_KEY [DEFAULT]
# - xai-default (grok-4-latest) - requires XAI_API_KEY  
# - anthropic-default (claude-4-sonnet) - requires ANTHROPIC_API_KEY

llm_providers:
  # Override built-in openai-default with different model
  openai-default:
    type: openai
    model: gpt-4  # Override default gpt-5
    api_key_env: OPENAI_API_KEY
    max_tool_result_tokens: 250000  # Conservative for 272K context
    
  # Add custom OpenAI proxy provider  
  openai-gemini-proxy:
    type: openai
    model: gemini-2.5-pro
    api_key_env: OPENAI_API_KEY
    base_url: https://my-openai-proxy.domain.com:443/v1beta/openai
    
  # Add custom provider with specific model
  gpt-4-turbo:
    type: openai
    model: gpt-4-turbo-preview
    api_key_env: OPENAI_API_KEY
    temperature: 0.0
    
  # Custom Gemini provider
  gemini-2.5-flash:
    type: google
    model: gemini-2.5-flash
    api_key_env: GOOGLE_API_KEY
    max_tool_result_tokens: 950000  # Conservative for 1M context
    enable_native_search: false  # Optional: Enable native search grounding (Google Search for Gemini, default: false)
    
  # Custom Grok provider
  grok-4:
    type: xai
    model: grok-4-latest
    api_key_env: XAI_API_KEY
    max_tool_result_tokens: 200000  # Conservative for 256K context
    
  # Custom Claude provider
  claude-4.1-opus:
    type: anthropic
    model: claude-4.1-opus
    api_key_env: ANTHROPIC_API_KEY
    max_tool_result_tokens: 150000  # Conservative for 200K context
    
  # Custom Vertex AI provider for Claude on GCP
  vertexai-claude:
    type: vertexai
    model: claude-sonnet-4-5@20250929  # Claude Sonnet 4.5
    api_key_env: VERTEX_AI_PROJECT  # Format: "project_id:location" or just "project_id"
    max_tool_result_tokens: 150000  # Conservative for 200K context
    # Note: Requires GOOGLE_APPLICATION_CREDENTIALS env var pointing to service account JSON

# Configuration Field Definitions:
# - type: Provider type (openai, google, xai, anthropic, vertexai) - maps to LLM_PROVIDERS function
# - model: Model name to use
# - api_key_env: Environment variable name containing API key
# - base_url: (Optional) Custom base URL, if not specified LangChain uses provider defaults
# - temperature: (Optional) Default temperature override
# - max_tool_result_tokens: (Optional) Maximum tokens for tool result content before LLM processing
# - enable_native_search: (Optional) Enable native search grounding (Google-only: enables Google Search for Gemini, default: false)
# 
# For Vertex AI (vertexai type):
#   - Requires GOOGLE_APPLICATION_CREDENTIALS environment variable pointing to service account JSON key
#   - api_key_env should contain: "project_id:location" (e.g., "my-project:us-east5") or just "project_id" (defaults to us-east5)
#   - Available Claude models: claude-sonnet-4-5@20250929, claude-opus-4@20250514

# Configuration Priority:
# 1. YAML providers (this file) - highest priority
# 2. Built-in defaults - fallback for providers not in YAML
# 3. Hardcoded fallback - if YAML file doesn't exist
