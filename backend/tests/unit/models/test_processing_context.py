"""
Unit tests for the new processing context models.

This module tests the clean context architecture that replaces the old
AlertProcessingData, IterationContext, PromptContext, and ChainExecutionContext.
"""

from unittest.mock import Mock

import pytest
from mcp.types import Tool
from pydantic import ValidationError

from tarsy.agents.prompts.builders import PromptBuilder
from tarsy.models.agent_execution_result import AgentExecutionResult
from tarsy.models.constants import StageStatus
from tarsy.models.processing_context import (
    AvailableTools,
    ChainContext,
    StageContext,
    ToolWithServer,
)


class TestAvailableTools:
    """Test AvailableTools model."""
    
    def test_available_tools_empty(self):
        """Test empty AvailableTools."""
        tools = AvailableTools()
        assert tools.tools == []
        
        # Use PromptBuilder to format tools
        builder = PromptBuilder()
        prompt_format = builder._format_available_actions(tools.tools)
        assert prompt_format == "No tools available."
    
    def test_available_tools_with_mcp_tools(self):
        """Test AvailableTools with ToolWithServer objects."""
        tool_with_server = ToolWithServer(
            server="k8s",
            tool=Tool(
                name="get_pods",
                description="Get Kubernetes pods",
                inputSchema={"type": "object", "properties": {}}
            )
        )
        
        tools = AvailableTools(tools=[tool_with_server])
        
        assert len(tools.tools) == 1
        assert isinstance(tools.tools[0], ToolWithServer)
        assert tools.tools[0].server == "k8s"
        assert tools.tools[0].tool.name == "get_pods"
        
        # Use PromptBuilder to format tools
        builder = PromptBuilder()
        prompt_format = builder._format_available_actions(tools.tools)
        assert "**k8s.get_pods**: Get Kubernetes pods" in prompt_format


class TestChainContext:
    """Test ChainContext model."""
    
    def test_chain_context_creation(self):
        """Test creating ChainContext with all required fields."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        processing_alert = ProcessingAlert(
            alert_type="kubernetes",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data={"pod": "failing-pod", "namespace": "default"}
        )
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="session-123",
            current_stage_name="analysis"
        )
        
        assert context.processing_alert.alert_type == "kubernetes"
        assert context.processing_alert.alert_data["pod"] == "failing-pod"
        assert context.session_id == "session-123"
        assert context.current_stage_name == "analysis"
        assert context.stage_outputs == {}
        assert context.runbook_content is None
        assert context.chain_id is None
    
    def test_chain_context_with_optional_fields(self):
        """Test ChainContext with optional fields populated."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        stage_result = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="TestAgent",
            timestamp_us=1234567890,
            result_summary="Stage completed successfully"
        )
        
        processing_alert = ProcessingAlert(
            alert_type="aws",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data={"instance_id": "i-123"}
        )
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="session-456",
            current_stage_name="investigation"
        )
        context.runbook_content = "# Investigation Runbook"
        context.chain_id = "chain-789"
        context.add_stage_result("previous_stage", stage_result)
        
        assert context.runbook_content == "# Investigation Runbook"
        assert context.chain_id == "chain-789"
        assert len(context.stage_outputs) == 1
        assert "previous_stage" in context.stage_outputs
    
    def test_get_original_alert_data(self):
        """Test getting original alert data as copy."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        original_data = {"key": "value", "number": 42}
        processing_alert = ProcessingAlert(
            alert_type="test",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data=original_data
        )
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="test-session",
            current_stage_name="test-stage"
        )
        
        retrieved_data = context.processing_alert.alert_data.copy()
        
        # Should be a copy, not the same object
        assert retrieved_data == original_data
        assert retrieved_data is not original_data
        
        # Modifying retrieved data shouldn't affect original
        retrieved_data["new_key"] = "new_value"
        assert "new_key" not in context.processing_alert.alert_data
    
    def test_get_runbook_content(self):
        """Test getting runbook content with defaults."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        # Test with no runbook content
        processing_alert = ProcessingAlert(
            alert_type="test",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data={"test": "data"}
        )
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="test-session",
            current_stage_name="test-stage"
        )
        assert context.get_runbook_content() == ""
        
        # Test with runbook content
        context.runbook_content = "# Test Runbook\nInstructions here"
        assert context.get_runbook_content() == "# Test Runbook\nInstructions here"
    
    def test_add_stage_result(self):
        """Test adding stage results."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        processing_alert = ProcessingAlert(
            alert_type="test",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data={"test": "data"}
        )
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="test-session",
            current_stage_name="current-stage"
        )
        
        result1 = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="Agent1",
            timestamp_us=1234567890,
            result_summary="First stage completed"
        )
        result2 = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="Agent2",
            timestamp_us=1234567891,
            result_summary="Second stage completed"
        )
        
        context.add_stage_result("stage1", result1)
        context.add_stage_result("stage2", result2)
        
        assert len(context.stage_outputs) == 2
        assert context.stage_outputs["stage1"] == result1
        assert context.stage_outputs["stage2"] == result2
    
    def test_get_previous_stages_results_empty(self):
        """Test getting previous stages when none exist."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        processing_alert = ProcessingAlert(
            alert_type="test",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data={"test": "data"}
        )
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="test-session",
            current_stage_name="first-stage"
        )
        
        results = context.get_previous_stages_results()
        assert results == []
    
    def test_get_previous_stages_results_with_completed_stages(self):
        """Test getting previous stages with completed results."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        processing_alert = ProcessingAlert(
            alert_type="test",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data={"test": "data"}
        )
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="test-session",
            current_stage_name="current-stage"
        )
        
        # Add completed stage
        completed_result = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="CompletedAgent",
            stage_name="completed_stage",
            timestamp_us=1234567890,
            result_summary="Completed stage result"
        )
        context.add_stage_result("completed_stage_exec_id", completed_result)
        
        # Add running stage (should be excluded)
        running_result = AgentExecutionResult(
            status=StageStatus.ACTIVE,
            agent_name="RunningAgent",
            stage_name="running_stage",
            timestamp_us=1234567891,
            result_summary="Running stage result"
        )
        context.add_stage_result("running_stage_exec_id", running_result)
        
        results = context.get_previous_stages_results()
        
        assert len(results) == 1
        assert results[0] == ("completed_stage", completed_result)
    
    def test_get_previous_stages_results_preserves_order(self):
        """Test that previous stages results preserve insertion order."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        processing_alert = ProcessingAlert(
            alert_type="test",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data={"test": "data"}
        )
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="test-session",
            current_stage_name="current-stage"
        )
        
        # Add stages in specific order
        result1 = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="Agent1",
            stage_name="stage1",
            timestamp_us=1234567890,
            result_summary="First stage completed"
        )
        result2 = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="Agent2",
            stage_name="stage2",
            timestamp_us=1234567891,
            result_summary="Second stage completed"
        )
        result3 = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="Agent3",
            stage_name="stage3",
            timestamp_us=1234567892,
            result_summary="Third stage completed"
        )
        
        context.add_stage_result("stage1_exec_id", result1)
        context.add_stage_result("stage2_exec_id", result2)
        context.add_stage_result("stage3_exec_id", result3)
        
        results = context.get_previous_stages_results()
        
        # Should maintain insertion order
        assert len(results) == 3
        assert results[0] == ("stage1", result1)
        assert results[1] == ("stage2", result2)
        assert results[2] == ("stage3", result3)
    
    def test_chain_context_field_validation(self):
        """Test field validation in ChainContext and ProcessingAlert."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        # Test empty alert_type in ProcessingAlert
        with pytest.raises(ValidationError):
            ProcessingAlert(
                alert_type="",
                severity="warning",
                timestamp=int(time.time() * 1_000_000),
                environment="production",
                runbook_url=None,
                alert_data={"test": "data"}
            )
        
        # Empty alert_data is now allowed (client might not have extra data)
        # Create valid ProcessingAlert with empty alert_data
        processing_alert = ProcessingAlert(
            alert_type="test",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data={}  # Empty is now allowed
        )
        # Should not raise
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="test-session",
            current_stage_name="test-stage"
        )
        assert context.processing_alert.alert_data == {}
        
        # Test empty session_id in ChainContext
        with pytest.raises(ValidationError):
            ChainContext.from_processing_alert(
                processing_alert=processing_alert,
                session_id="",
                current_stage_name="test-stage"
            )
        
        # Test empty current_stage_name in ChainContext
        with pytest.raises(ValidationError):
            ChainContext.from_processing_alert(
                processing_alert=processing_alert,
                session_id="test-session",
                current_stage_name=""
            )


class TestStageContext:
    """Test StageContext model."""
    
    def create_test_chain_context(self) -> ChainContext:
        """Create a test ChainContext for StageContext tests."""
        import time

        from tarsy.models.alert import ProcessingAlert
        
        processing_alert = ProcessingAlert(
            alert_type="kubernetes",
            severity="warning",
            timestamp=int(time.time() * 1_000_000),
            environment="production",
            runbook_url=None,
            alert_data={"pod": "test-pod", "namespace": "default"}
        )
        context = ChainContext.from_processing_alert(
            processing_alert=processing_alert,
            session_id="test-session-123",
            current_stage_name="analysis"
        )
        context.runbook_content = "# Test Runbook\nAnalyze the pod failure."
        context.chain_id = "test-chain"
        return context
    
    def create_test_available_tools(self) -> AvailableTools:
        """Create test AvailableTools for StageContext tests."""
        tool_with_server = ToolWithServer(
            server="k8s",
            tool=Tool(
                name="get_pods",
                description="Get pod information",
                inputSchema={"type": "object", "properties": {}}
            )
        )
        return AvailableTools(tools=[tool_with_server])
    
    def create_mock_agent(self) -> Mock:
        """Create a mock BaseAgent for testing."""
        agent = Mock()
        agent.__class__.__name__ = "TestAgent"
        agent.mcp_servers.return_value = ["k8s-server", "aws-server"]
        return agent
    
    def test_stage_context_creation(self):
        """Test creating StageContext with all components."""
        chain_context = self.create_test_chain_context()
        available_tools = self.create_test_available_tools()
        agent = self.create_mock_agent()
        
        stage_context = StageContext(
            chain_context=chain_context,
            available_tools=available_tools,
            agent=agent
        )
        
        assert stage_context.chain_context == chain_context
        assert stage_context.available_tools == available_tools
        assert stage_context.agent == agent
    
    def test_stage_context_derived_properties(self):
        """Test StageContext derived properties."""
        chain_context = self.create_test_chain_context()
        available_tools = self.create_test_available_tools()
        agent = self.create_mock_agent()
        
        stage_context = StageContext(
            chain_context=chain_context,
            available_tools=available_tools,
            agent=agent
        )
        
        # Test property access
        assert stage_context.alert_data == {"pod": "test-pod", "namespace": "default"}
        assert stage_context.runbook_content == "# Test Runbook\nAnalyze the pod failure."
        assert stage_context.session_id == "test-session-123"
        assert stage_context.stage_name == "analysis"
        assert stage_context.agent_name == "TestAgent"
        assert stage_context.mcp_servers == ["k8s-server", "aws-server"]
    
    def test_stage_context_alert_data_is_copy(self):
        """Test that alert_data property returns a copy."""
        chain_context = self.create_test_chain_context()
        available_tools = self.create_test_available_tools()
        agent = self.create_mock_agent()
        
        stage_context = StageContext(
            chain_context=chain_context,
            available_tools=available_tools,
            agent=agent
        )
        
        alert_data = stage_context.alert_data
        alert_data["modified"] = "value"
        
        # Original data should not be modified
        assert "modified" not in chain_context.processing_alert.alert_data
    
    def test_stage_context_previous_stages_empty(self):
        """Test StageContext with no previous stages."""
        chain_context = self.create_test_chain_context()
        available_tools = self.create_test_available_tools()
        agent = self.create_mock_agent()
        
        stage_context = StageContext(
            chain_context=chain_context,
            available_tools=available_tools,
            agent=agent
        )
        
        assert stage_context.previous_stages_results == []
        assert not stage_context.has_previous_stages()
        assert stage_context.format_previous_stages_context() == "No previous stage context available."
    
    def test_stage_context_with_previous_stages(self):
        """Test StageContext with previous completed stages."""
        chain_context = self.create_test_chain_context()
        
        # Add completed previous stages
        result1 = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="DataAgent",
            stage_name="collection",
            stage_description="Data Collection",
            timestamp_us=1234567890,
            result_summary="Collected system metrics and logs."
        )
        
        result2 = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="AnalysisAgent",
            stage_name="initial_analysis",
            stage_description="Initial Analysis",
            timestamp_us=1234567891,
            result_summary="Identified potential CPU bottleneck."
        )
        
        chain_context.add_stage_result("collection_exec_id", result1)
        chain_context.add_stage_result("initial_analysis_exec_id", result2)
        
        available_tools = self.create_test_available_tools()
        agent = self.create_mock_agent()
        
        stage_context = StageContext(
            chain_context=chain_context,
            available_tools=available_tools,
            agent=agent
        )
        
        # Test previous stages access
        previous = stage_context.previous_stages_results
        assert len(previous) == 2
        assert previous[0] == ("collection", result1)
        assert previous[1] == ("initial_analysis", result2)
        assert stage_context.has_previous_stages()
        
        # Test formatted context
        formatted = stage_context.format_previous_stages_context()
        assert "## Results from 'Data Collection' stage:" in formatted
        assert "Collected system metrics and logs." in formatted
        assert "## Results from 'Initial Analysis' stage:" in formatted
        assert "Identified potential CPU bottleneck." in formatted
    
    def test_stage_context_formatted_context_with_missing_description(self):
        """Test formatted context when stage_description is missing."""
        chain_context = self.create_test_chain_context()
        
        result = AgentExecutionResult(
            status=StageStatus.COMPLETED,
            agent_name="UnnamedAgent",
            stage_name="unnamed_stage",
            stage_description=None,  # Missing description
            timestamp_us=1234567890,
            result_summary="Stage completed successfully."
        )
        
        chain_context.add_stage_result("unnamed_stage_exec_id", result)
        
        available_tools = self.create_test_available_tools()
        agent = self.create_mock_agent()
        
        stage_context = StageContext(
            chain_context=chain_context,
            available_tools=available_tools,
            agent=agent
        )
        
        formatted = stage_context.format_previous_stages_context()
        assert "## Results from 'unnamed_stage' stage:" in formatted
        assert "Stage completed successfully." in formatted


