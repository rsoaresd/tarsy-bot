# Tarsy - Environment Configuration Template
# Copy this file to .env and fill in your actual values

# =============================================================================
# Application Settings
# =============================================================================
# LLM provider to use
# Built-in providers available out-of-the-box:
# - openai-default (gpt-5)
# - google-default (gemini-2.5-pro) [DEFAULT]
# - xai-default (grok-4-latest)  
# - anthropic-default (claude-4-sonnet)
# - vertexai-default (claude-sonnet-4-5)
LLM_PROVIDER=google-default

# =============================================================================
# LLM Provider Configuration
# =============================================================================
# Built-in providers available out-of-the-box:
# OOTB: Just set GOOGLE_API_KEY to use google-default (gemini-2.5-pro)
# Or override with LLM_PROVIDER:
# - openai-default (gpt-5)
# - google-default (gemini-2.5-pro) [DEFAULT]
# - xai-default (grok-4-latest)  
# - anthropic-default (claude-4-sonnet)
# - vertexai-default (claude-sonnet-4-5)

# OOTB setup - just set this and you're ready to go:
GOOGLE_API_KEY=your_google_api_key_here

# Optional: Override LLM provider
# LLM_PROVIDER=openai-default

# Optional: Path to custom LLM providers configuration file
# LLM_CONFIG_PATH=./config/llm_providers.yaml

# =============================================================================
# OPTIONAL: Additional LLM Provider API Keys
# =============================================================================
# Only set these if you want to use other providers or override the default

# OpenAI API Key  
# Get from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=your_openai_api_key_here

# X.AI API Key (for Grok models)
# Get from: https://console.x.ai/
# XAI_API_KEY=your_xai_api_key_here

# Anthropic API Key (for Claude models)
# Get from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Vertex AI (for Claude models via Vertex AI)
# Authentication via service account (GOOGLE_APPLICATION_CREDENTIALS)
# Get from: https://console.cloud.google.com/vertex-ai

# GCP Project ID (required) - Standard GCP environment variable
# GOOGLE_CLOUD_PROJECT=your-project-id

# GCP Region (optional, defaults to us-east5) - Standard GCP environment variable
# GOOGLE_CLOUD_LOCATION=us-east5

# Path to service account JSON key file (required for Vertex AI)
# Download from: https://console.cloud.google.com/iam-admin/serviceaccounts
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json

# =============================================================================
# REQUIRED: GitHub Configuration
# =============================================================================
# Required for downloading runbooks from repositories
# Get from: https://github.com/settings/tokens
# Permissions needed: repo (for private repos) or public_repo (for public repos)
GITHUB_TOKEN=your_github_token_here

# Optional: GitHub repository URL for runbooks list
# The dashboard will fetch runbook URLs from this repository
# Format: https://github.com/org/repo/tree/branch/path
# Example: https://github.com/codeready-toolchain/sandbox-sre/tree/master/runbooks/ai
# Note: Private repos require GITHUB_TOKEN to be set above
# RUNBOOKS_REPO_URL=https://github.com/your-org/your-repo/tree/master/runbooks

# =============================================================================
# MCP Server Template Variables
# =============================================================================
# These variables are used by MCP server configurations with ${VARIABLE_NAME} syntax
# Set the ones you need based on your configured MCP servers
#
# NOTE: Only KUBECONFIG has a built-in default. All other template variables 
# must be explicitly set in this file or your environment.

# Kubernetes Configuration
# Path to your kubeconfig file for the built-in kubernetes-server
# Default: Expanded absolute path (e.g., /home/user/.kube/config) - tilde is automatically expanded
# When setting custom paths, always use absolute paths - tilde expansion only works for built-in defaults
# KUBECONFIG=/path/to/your/kubeconfig

# Monitoring Configuration (examples - no defaults, must be set explicitly)
PROMETHEUS_URL=http://your-prometheus:9090
GRAFANA_URL=http://your-grafana:3000

# Custom MCP Server Tokens/Credentials (examples from agents.yaml.example)
# These must be set explicitly
SECURITY_SERVER_TOKEN=your_security_server_token_here
GRAFANA_TOKEN=your_grafana_api_token_here

# AWS Configuration (if using aws-server)
AWS_ACCESS_KEY_ID=your_aws_access_key_id
AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key
AWS_DEFAULT_REGION=us-west-2

# ArgoCD Configuration (for future argocd-server)
# ARGOCD_SERVER=your-argocd-server-url
# ARGOCD_TOKEN=your_argocd_auth_token

# =============================================================================
# Alert Data Masking Configuration
# =============================================================================
# Enable/disable masking of sensitive data in incoming alerts (default: true)
# When enabled, patterns like API keys, passwords, tokens, certificates, etc. will be masked
# ALERT_DATA_MASKING_ENABLED=true

# Pattern group to use for alert data masking (default: security)
# Available groups: basic, secrets, security, kubernetes, all
# - basic: api_key, password
# - secrets: api_key, password, token
# - security: api_key, password, token, certificate, certificate_authority_data, email, ssh_key
# - kubernetes: kubernetes_secret (code-based masker), api_key, password, certificate_authority_data (masks Secrets only, not ConfigMaps)
# - all: all available patterns
# ALERT_DATA_MASKING_PATTERN_GROUP=security

# =============================================================================
# Database Configuration
# =============================================================================
# History database URL - automatically defaults based on environment:
# - Tests: sqlite:///:memory: (in-memory, isolated)
# - Dev/Prod: sqlite:///history.db (persistent file)
# Override if you need custom database configuration
# HISTORY_DATABASE_URL=sqlite:///custom_history.db
# HISTORY_DATABASE_URL=postgresql://user:pass@localhost:5432/tarsy_history


# History retention in days (default: 365)
# HISTORY_RETENTION_DAYS=365

# History cleanup interval in hours (default: 12)
# How often to run automatic cleanup of old history data
# HISTORY_CLEANUP_INTERVAL_HOURS=12

# =============================================================================
# JWT Authentication Configuration
# =============================================================================
# Path to JWT public key file for API token validation
# This is used by the JWKS endpoint (/.well-known/jwks.json) to serve the public key
# for oauth2-proxy and other JWT token validators
# Default: ../config/keys/jwt_public_key.pem (relative to backend directory)
# JWT_PUBLIC_KEY_PATH=../config/keys/jwt_public_key.pem

# For production, you might want to use an absolute path:
# JWT_PUBLIC_KEY_PATH=/etc/tarsy/keys/jwt_public_key.pem

# =============================================================================
# Multi-Layer Agent Architecture Configuration
# =============================================================================
# The agent registry and MCP server registry are configured in settings.py
# with sensible defaults. You can override them here if needed.

# Agent Registry - Maps alert types to specialized agent classes
# Example override (JSON format):
# AGENT_REGISTRY='{"NamespaceTerminating": "KubernetesAgent", "ArgoCD Sync Failed": "ArgoCDAgent"}'

# MCP Server Registry - Single source of truth for all MCP server configurations
# Note: This replaces the old mcp_servers configuration
# Example override (JSON format):
# MCP_SERVER_REGISTRY='{"kubernetes-server": {"server_type": "kubernetes", "enabled": true, "transport": {"command": "npx", "args": ["-y", "kubernetes-mcp-server@0.0.54"]}, "instructions": "For Kubernetes operations..."}}'

# Configuration-Based Agents (EP-0006)
# Path to YAML file defining agents and MCP servers (default: ./config/agents.yaml)
# Copy config/agents.yaml.example to config/agents.yaml and customize as needed
# AGENT_CONFIG_PATH=./config/agents.yaml

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Server host and port configuration
HOST=0.0.0.0
PORT=8000

# =============================================================================
# CORS Configuration
# =============================================================================
# Allowed origins for CORS (comma-separated list)
# For development, include your alert dev UI URL
# For production, use your actual domain
CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173,http://localhost:3000,http://127.0.0.1:3000


# =============================================================================
# Slack Notification Configuration
# =============================================================================

# Slack Bot token for sending notifications
# 1. Create a Slack App, if you do not have it: https://api.slack.com/apps
# 2. In your app's OAuth & Permissions, under Bot Token Scopes, add the following ones:
#    - channels:history (read messages in public channels)
#    - groups:history (read messages in private channels)
#    - chat:write (post messages)
#    - channels:read (view basic channel info)
#    - groups:read (view basic private channel info)
# 3. Under OAuth Tokens, click on "install" (or "reinstall")
# 4. Invite your bot to your channel: /invite @your-bot
# 5. Copy the Bot User OAuth Token (starts with xoxb-)
SLACK_BOT_TOKEN=your-slack-bot-token

# Slack channel to send notifications to
SLACK_CHANNEL=your-slack-channel


# =============================================================================
# QUICK START INSTRUCTIONS
# =============================================================================
# 
# 1. Copy this file to .env:
#    cp env.template .env
#
# 2. Edit .env and set these REQUIRED variables:
#    - At least one LLM API key (GOOGLE_API_KEY, OPENAI_API_KEY, XAI_API_KEY, or ANTHROPIC_API_KEY)
#    - GITHUB_TOKEN for runbook access
#    - For ultra-easy setup: Just set GOOGLE_API_KEY and you're ready to go!
#
# 3. Optional: Set up Kubernetes MCP Server on localhost:8080
#
# 4. Start the backend:
#    uvicorn tarsy.main:app --reload --port 8000
#
# =============================================================================

# =============================================================================
# Optional: Advanced Configuration
# =============================================================================

# LLM Model Configuration
# GEMINI_MODEL=gemini-2.5-pro
# OPENAI_MODEL=gpt-4
# GROK_MODEL=grok-3

# SSL Verification for LLM API calls
# WARNING: Only disable SSL verification if you're using internal proxies with self-signed certificates or during testing
# DISABLE_SSL_VERIFICATION=false

# Request Timeouts (seconds)
# REQUEST_TIMEOUT=30
# LLM_TIMEOUT=60
# MCP_TIMEOUT=30

# Alert Queue Configuration
# Maximum concurrent alerts across ALL pods (global limit, not per-pod)
# Enforced via database-backed queue with SessionClaimWorker
MAX_CONCURRENT_ALERTS=5

# Optional: Maximum queue size (reject new alerts when queue is full)
# MAX_QUEUE_SIZE=100

# Optional: Queue claim retry interval
# QUEUE_CLAIM_INTERVAL_SECONDS=1.0

# Alert processing timeout (seconds)
ALERT_PROCESSING_TIMEOUT=900      # Timeout (seconds) for processing a single alert (default: 15 minutes)

# Runbook Configuration
# MAX_RUNBOOK_SIZE_MB=10

# Force LLM to conclude when max iterations reached (instead of pausing)
# Set to 'true' to enable forced conclusions, 'false' for pause/resume behavior
# Note: Follow-up chats always force conclusion regardless of this setting
# FORCE_CONCLUSION_AT_MAX_ITERATIONS=false

# GitHub API Configuration
# GITHUB_API_URL=https://api.github.com
# GITHUB_RAW_URL=https://raw.githubusercontent.com

# WebSocket Configuration
# WS_PING_INTERVAL=20
# WS_PING_TIMEOUT=10
# WS_MAX_CONNECTIONS=100

# =============================================================================
# Development/Testing Settings
# =============================================================================

# Enable verbose debug logging
# DEBUG_MODE=false

# Use mock responses for testing without real services
# USE_MOCK_MCP=false
# USE_MOCK_LLM=false

# Force testing mode (automatically detected when running pytest)
# TESTING=true

# Cache Configuration
# RUNBOOK_CACHE_TTL=3600
# RUNBOOK_CACHE_DIR=/tmp/sre-runbooks
# ENABLE_RESPONSE_CACHE=true

# =============================================================================
# Production Settings
# =============================================================================

# Security
# SECRET_KEY=your-super-secret-key-for-production
# ALLOWED_HOSTS=your-domain.com,api.your-domain.com

# Database (optional - for persistent storage)
#
# IMPORTANT: Passwords with special characters (@, #, $, %, &, etc.) must be URL-encoded
# when using DATABASE_URL directly. Use one of these approaches:
#
# APPROACH 1 (Recommended): Use separate components (automatic encoding)
# DATABASE_USER=tarsy
# DATABASE_PASSWORD=my-p@ssw0rd  # No encoding needed!
# DATABASE_HOST=localhost
# DATABASE_PORT=5432
# DATABASE_NAME=tarsy
#
# APPROACH 2: Use encoded DATABASE_URL (manual encoding required)
# Use: python backend/encode_db_password.py "my-p@ssw0rd" to encode your password
# DATABASE_URL=postgresql://user:my-p%40ssw0rd@localhost:5432/tarsy
#
# Simple passwords without special characters can be used directly:
# DATABASE_URL=postgresql://user:simplepassword@localhost:5432/tarsy

# Redis (optional - for caching and task queues)  
# REDIS_URL=redis://localhost:6379/0

# SSL/TLS Configuration
# SSL_KEYFILE=/path/to/ssl/private.key
# SSL_CERTFILE=/path/to/ssl/certificate.crt

# Rate Limiting
# RATE_LIMIT_ENABLED=true
# RATE_LIMIT_PER_MINUTE=60
# RATE_LIMIT_BURST=10

# Monitoring and Metrics
# ENABLE_METRICS=true
# METRICS_PORT=9090
# HEALTH_CHECK_INTERVAL=30

# =============================================================================
# Example Test Values
# =============================================================================
# Use these for initial testing and development

# Example cluster URL (replace with your actual cluster)
# TEST_CLUSTER_URL=https://api.example-cluster.com:6443

# Example namespace
# TEST_NAMESPACE=kube-system

# Example alert message
# TEST_ALERT_MESSAGE=namespace is stuck in 'Terminating' phase

# Default test runbook
# TEST_RUNBOOK_URL=https://github.com/codeready-toolchain/sandbox-sre/blob/master/runbooks/namespace-terminating.md

# =============================================================================
# Environment-Specific Configuration
# =============================================================================
# You can create multiple environment files:
# - .env.development
# - .env.staging  
# - .env.production
# - .env.test (for manual test database override)
#
# Load specific environment with:
# uvicorn tarsy.main:app --env-file .env.production

# =============================================================================
# Troubleshooting
# =============================================================================
# 
# Common issues and solutions:
#
# 1. "LLM provider not available" error:
#    - Check that at least one API key is set correctly
#    - Verify the API key has sufficient quota/credits
#    - Test the API key with a simple request
#
# 2. "Failed to download runbook" error:
#    - Check GITHUB_TOKEN has correct permissions
#    - Verify the runbook URL is accessible
#    - Test with a public repository first
#
# 3. "MCP server not available" error:
#    - Check KUBERNETES_MCP_URL is correct
#    - Verify the MCP server is running and responding
#    - Use the mock server script for testing
#
# 4. Alert Dev UI connection issues:
#    - Check CORS_ORIGINS includes your alert dev UI URL
#    - Verify backend is running on the correct port
#    - Check browser developer console for CORS errors
#
# 5. Test database pollution:
#    - Tests automatically use in-memory databases (sqlite:///:memory:)
#    - If you see shared database issues, check that TESTING=true during tests
#    - Clean up any *.db files in the backend directory if needed
